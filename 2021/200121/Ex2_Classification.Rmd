---
title: "2021/01/20 Ex.2"
author: "Marco Scarpelli"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list=ls())
graphics.off()

library(MASS)      #for QDA 
library(rgl)       #for 3D visualization
library(mvtnorm)   #for multivariate normality
library(mvnormtest)   #for multivariate normality
library(MVN)   #for multivariate normality
library(class)     #for KNN

set.seed(20240605)

data <- read.table('activity.txt', header=T)
```

# Dataset exploration
```{r echo = FALSE} 
head(data)
dim(data)
```

We will import and jitter each variable separately, with a standard deviation that is 1% of the variable's mean.
```{r echo = FALSE}
# Questo tipo di cella viene eseguita ma nè il suo output, nè il suo codice vengono mostrati

group<-factor(data$activity)

class.A<- levels(group)[1] # name of class A
class.B<- levels(group)[2] # name of class B
class.C<- levels(group)[3] # name of class C

iA <- which(data$activity==levels(group)[1])  
iB <- which(data$activity==levels(group)[2])  
iC <- which(data$activity==levels(group)[3])  

data<-data[,1:2] #keep the quantitative columns

n<-dim(data)[1]
p<-dim(data)[2]

data$accel <- data$accel + cbind(rnorm(n, sd=sapply(data$accel, mean)*0.01)) # jittering

data$gyro <- data$gyro + cbind(rnorm(n, sd=sapply(data$gyro, mean)*0.01)) # jittering

col.lab<-rep(0,n)
col.lab[iA]<-'red'
col.lab[iB]<-'blue'
col.lab[iC]<-'green'

plot(data, col=col.lab, main='', xlab='', ylab='', pch=19)
legend("topright", legend=levels(group), fill=c('red','blue', 'green'), lty=c(1,1)) #only in plot

```

# Point a
## Assumptions
We want multivariate normality within the groups:
```{r echo = FALSE}
mvn(data[iA,])$multivariateNormality
mvn(data[iB,])$multivariateNormality
mvn(data[iC,])$multivariateNormality
```

All tests report normality. Let us check how the covariance structure is, to decide whether we should use LDA or QDA:
```{r echo = FALSE}
SA<-cov(data[iA,])
SB<-cov(data[iB,])
SC<-cov(data[iC,])

par(mfrow=c(1,3))
image(SA, col=heat.colors(100),main='Cov. SA', asp=1, axes = FALSE, breaks = quantile(rbind(SA,SB,SC), (0:100)/100, na.rm=TRUE))

image(SB, col=heat.colors(100),main='Cov. SB', asp=1, axes = FALSE, breaks = quantile(rbind(SA,SB,SC), (0:100)/100, na.rm=TRUE))

image(SC, col=heat.colors(100),main='Cov. SB', asp=1, axes = FALSE, breaks = quantile(rbind(SA,SB,SC), (0:100)/100, na.rm=TRUE))
par(mfrow=c(1,1))
```

We will print them to check numerically:
```{r echo = FALSE}
SA
SB
SC
```

To use LDA, we should observe that no value on the diagonal of each matrix is greater than 4 times larger than the corresponding diagonal values on the other matrices. In this case, we cannot use LDA.

## QDA
Means within groups:
```{r echo = FALSE}
nA <- length(iA)
nB <- length(iB)
nC <- length(iC)

pA <- nA / n
pB <- nB / n
pC <- nC / n

priors<-c(pA, pB, pC)

df.qda <- qda(data, group, prior=priors)

Qda.on.df <- predict(df.qda, data)

cluster.pred.A<-which(Qda.on.df$class==class.A)
cluster.pred.B<-which(Qda.on.df$class==class.B)
cluster.pred.C<-which(Qda.on.df$class==class.C)

mA<-colMeans(data[iA,])
mB<-colMeans(data[iB,])
mC<-colMeans(data[iC,])
mA
mB
mC
```

Classification regions:

```{r echo = FALSE}
x  <- seq(min(data[,1]), max(data[,1]), length=200)
y  <- seq(min(data[,2]), max(data), length=200)
xy <- expand.grid(accel=x, gyro=y)

z  <- predict(df.qda, xy)$post
z1 <- z[,1] - pmax(z[,2], z[,3])
z2 <- z[,2] - pmax(z[,1] , z[,3])
z3 <- z[,3] - pmax(z[,1], z[,2])
plot(data, col=col.lab,main='', xlab='', ylab='', pch=19)
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)  
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z3, 200), levels=0, drawlabels=F, add=T)
legend("topright", legend=levels(group), fill=c('red','blue', 'green'), lty=c(1,1)) #only in plot
points(df.qda$means, pch=4,col=c('red','blue', 'green') , lwd=2, cex=1.5)

```

# Point B
```{r echo = FALSE}
errors<-sum(Qda.on.df$class!=group)
APER<-errors/length(group)
APER
```

# Point C
```{r echo = FALSE}
s0.new<- data.frame(accel = 0.45, gyro= 0.52)
predict(df.qda, s0.new)     
```

The new point is identified by a black cross:

```{r echo = FALSE}
plot(data, col=col.lab,main='', xlab='', ylab='', pch=19)
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)  
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z3, 200), levels=0, drawlabels=F, add=T)
legend("topright", legend=levels(group), fill=c('red','blue', 'green'), lty=c(1,1)) #only in plot
points(df.qda$means, pch=4,col=c('red','blue', 'green') , lwd=2, cex=1.5)

points(s0.new[1],s0.new[2], col='black', pch=4,lwd=2, cex=1.5)

```

# Point D
We classify the dataset and report the resulting table:
```{r echo = FALSE}
k <- 5
data.knn <- knn(train = data, test = data, cl = group, k = k)
misc<-table(data.knn, group)
misc
```

The APER is:
```{r echo = FALSE}
errors<-data.knn!=group 
APERknn<-sum(errors)/n
APERknn
```

The two classifiers perform similarly, with the KNN one being slightly worse.