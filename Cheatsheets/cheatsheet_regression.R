###---------------###
### LINEAR MODELS ###------------------------------------------------------
###---------------###

### Try It Out! ###
rm(list = ls())
graphics.off()
par(mfrow=c(1,1))
cat("\014")
data <- read.table('datasets/data_regression.txt', header = T)
### Try It Out! ###

# Fast way to write the formula (copy-paste the names row)

head(data)
n <- dim(data)[1]

#### Parameters Estimation ----

m0 <- lm(target ~ reg1 + reg2 + reg3 + dummy, data = data)
summary(m0)
# Note: pay attention when dummy assumes numeric values;
# factorize it: as.factor(dummy)

m0$fitted        # y hat (i.e. cbind(rep(1, n), data) %*% m0$coef
m0$residuals     # eps hat

m0$coefficients  # beta_i
vcov(m0)         # cov(beta_i)

m0$rank # order of the model [r+1]
m0$df   # degrees of freedom of the residuals [n-(r+1)]

hatvalues(m0) # h_ii (or sometimes called "leverage")
# They quantify:
# 1) How far is the i-th observation from the other ones in the features space
# 2) The influence of the i-th observation on the fit (can be seen as the
# derivative dyhat_i / dy_i)

rstandard(m0) # standardized residuals: eps_j / sqrt(s^2*(1-h_ii))

sum((m0$residuals)^2)/m0$df  # s^2 estimate of sigma^2
summary(m0)$sigma^2 # equivalent

AIC(m0) # a metric that is used to quantify the fit of models (the lower the better)

summary(m0)$r.squared # R^2
summary(m0)$adj.r.squared

SStot <- sum((data$target - mean(data$target))^2)
SSfit <- sum((m0$fitted - mean(data$target))^2)
SSres <- sum((m0$residuals)^2)

SSfit / SStot # percentage of explained variability
summary(m0)$r.squared # equivalent

SSres / SStot # percentage of unexplained variability
1 - summary(m0)$r.squared # equivalent

# The above about R-squared only works when a column full of "1" belongs to the space generated by the design matrix
# (i.e. this is for sure when we fit the model with a global intercept -> no ~ -1),
# otherwise we have a collapse of the geometrical interpretation of R-squared:
# we can compute it but it has no meaning (it may turn out negative or infinite...)

# If we wanted the model to pass through the origin (i.e. ~ -1), 
# in order to have a measure of fit, we need to calculate the variability around 0 and not around the mean

SStot.z <- sum(data$target^2)
SSfit.z <- sum(m0$fitted^2)
SSres <- sum((m0$residuals)^2)

SSfit.z / SStot.z # percentage of explained variability
summary(m0)$r.squared # equivalent

SSres / SStot.z # percentage of unexplained variability
1 - summary(m0)$r.squared # equivalent

# Models Comparison

AIC(m0, m1) # the lower the better
anova(m0, m1) # the lower the p-value, the more the bigger model is preferable (it can be computed when one model is nested in the other one)


#### Inference on the Parameters ----

alpha <- 0.05
r <- length(m0$coefficients) - 1 # number of regressors
df <- m0$df # n - (r+1)

# H0: (beta2, beta4) == (0, 0) vs H1: (beta2, beta4) != (0, 0)

C <- rbind(c(0,0,1,0,0),
           c(0,0,0,0,1))

linearHypothesis(m0, C, c(0,0)) #!library(car)

p <- 2  # number of tested coefficients

# Confidence Region

semiaxes.length <- sqrt(p*qf(1-alpha, p, df)) * sqrt(eigen(vcov(m0)[c(3, 5), c(3, 5)])$values)

plot(m0$coefficients[3], m0$coefficients[5], 
     xlim = c(m0$coefficients[3] - 1.5 * max(semiaxes.length), m0$coefficients[3] + 1.5 * max(semiaxes.length)), asp = 1, 
     xlab = 'beta[2]', ylab = 'beta[4]')

abline(h = 0, v = 0, col = 'grey35', lty = 2)
points(0, 0, col = 'red', pch = 9, cex = 1)

ellipse(c(m0$coefficients[3], m0$coefficients[5]), vcov(m0)[c(3, 5), c(3, 5)], sqrt(p*qf(1-alpha, p, df)), lty = 2, col = 'blue')

center <- C %*% m0$coefficients
shape <- C %*% vcov(m0) %*% t(C)

# Simultaneous T2 intervals

cfr.fisher <- p*qf(1-alpha, p, df)

T2.I <- cbind(center - sqrt(cfr.fisher * diag(shape)), 
              center, 
              center + sqrt(cfr.fisher * diag(shape)))
colnames(T2.I) <- c('inf', 'center', 'sup')
rownames(T2.I) <- c(names(m0$coefficients[which(C[1, ] == 1)]), names(m0$coefficients[which(C[2, ] == 1)]))
T2.I

# or

T2.I <- rbind(c(m0$coefficients[3] - sqrt(cfr.fisher * vcov(m0)[3, 3]),
                m0$coefficients[3],
                m0$coefficients[3] + sqrt(cfr.fisher * vcov(m0)[3, 3])),
              
              c(m0$coefficients[5] - sqrt(cfr.fisher * vcov(m0)[5, 5]),
                m0$coefficients[5],
                m0$coefficients[5] + sqrt(cfr.fisher * vcov(m0)[5, 5])))
colnames(T2.I) <- c('inf', 'center', 'sup')
rownames(T2.I) <- c(names(m0$coefficients[c(3, 5)]))
T2.I

# Bonferroni intervals 

qT <- qt(1 - alpha/(2*p), df)

BF.I <- cbind(center - sqrt(diag(shape)) * qT,
              center,
              center + sqrt(diag(shape)) * qT)
colnames(BF.I) <- c('inf', 'center', 'sup')
rownames(BF.I) <- c(names(m0$coefficients[which(C[1, ] == 1)]), names(m0$coefficients[which(C[2, ] == 1)]))
BF.I

# or 

BF.I <- rbind(c(m0$coefficients[3] - sqrt(vcov(m0)[3, 3]) * qT,
                m0$coefficients[3],
                m0$coefficients[3] + sqrt(vcov(m0)[3, 3]) * qT),
              
              c(m0$coefficients[5] - sqrt(vcov(m0)[5, 5]) * qT,
                m0$coefficients[5],
                m0$coefficients[5] + sqrt(vcov(m0)[5, 5]) * qT))
colnames(BF.I) <- c('inf', 'center', 'sup')
rownames(BF.I) <- c(names(m0$coefficients[c(3, 5)]))
BF.I

# or (only for intervals on beta)

confint(m0, level = 1-alpha/p)[c(3, 5), ]  # Bonferroni correction!
# Note: confint() returns the confidence intervals one-at-a-time;
# to have a global level 95% we need to include a correction

abline(v = T2.I[1, 1], col = 'orange', lwd = 1, lty = 2)
abline(v = T2.I[1, 3], col = 'orange', lwd = 1, lty = 2)
abline(h = T2.I[2, 1], col = 'orange', lwd = 1, lty = 2)
abline(h = T2.I[2, 3], col = 'orange', lwd = 1, lty = 2)

segments(T2.I[1, 1], 0, T2.I[1, 3], 0, lty = 1, lwd = 2, col = 'orange')
segments(0, T2.I[2, 1], 0, T2.I[2, 3], lty = 1, lwd = 2, col = 'orange')

abline(v = BF.I[1, 1], col = 'purple', lwd = 1, lty = 2)
abline(v = BF.I[1, 3], col = 'purple', lwd = 1, lty = 2)
abline(h = BF.I[2, 1], col = 'purple', lwd = 1, lty = 2)
abline(h = BF.I[2, 3], col = 'purple', lwd = 1, lty = 2)

segments(BF.I[1, 1], 0, BF.I[1, 3], 0, lty = 1, lwd = 2, col = 'purple')
segments(0, BF.I[2, 1], 0, BF.I[2, 3], lty = 1, lwd = 2, col = 'purple')

legend('topright', c('Bonf. CI', 'Sim-T2 CI'), col = c('purple', 'orange'), lty = 1, lwd = 2)

# H0: (beta2+beta4, beta3) == (0,0) vs H1: (beta2+beta4, beta3) != (0,0)

C <- rbind(c(0,0,1,0,1),
           c(0,0,0,1,0))

linearHypothesis(m0, C, c(0,0))

p <- 2  # number of tested coefficients

# Confidence Region

center <- C %*% m0$coefficients
shape <- C %*% vcov(m0) %*% t(C)

semiaxes.length <- sqrt(p*qf(1-alpha, p, df)) * sqrt(eigen(shape)$values)

plot(center[1], center[2], 
     xlim = c(center[1] - 1.5 * max(semiaxes.length), center[1] + 1.5 * max(semiaxes.length)), asp = 1, 
     xlab = 'beta[2]+beta[4]', ylab = 'beta[3]')

abline(h = 0, v = 0, col = 'grey35', lty = 2)
points(0, 0, col = 'red', pch = 9, cex = 1)

ellipse(c(center), shape, sqrt(p*qf(1-alpha, p, df)), lty = 2, col = 'blue')

# Simultaneous T2 intervals

cfr.fisher <- p*qf(1-alpha, p, df)

T2.I <- cbind(center - sqrt(cfr.fisher * diag(shape)), 
              center, 
              center + sqrt(cfr.fisher * diag(shape)))
colnames(T2.I) <- c('inf', 'center', 'sup')
T2.I

# or

T2.I <- rbind(beta2.4 = c(center[1] - sqrt(cfr.fisher * shape[1, 1]),
                          center[1],
                          center[1] + sqrt(cfr.fisher * shape[1, 1])),
              
              beta3 = c(center[2] - sqrt(cfr.fisher * shape[2, 2]),
                        center[2],
                        center[2] + sqrt(cfr.fisher * shape[2, 2])))
colnames(T2.I) <- c("inf", "center", "sup")
T2.I

# or 

T2.I <- rbind(beta2.4 = c(center[1] - sqrt(cfr.fisher * t(C[1, ]) %*% vcov(m0) %*% C[1, ]),
                          center[1],
                          center[1] + sqrt(cfr.fisher * t(C[1, ]) %*% vcov(m0) %*% C[1, ])),
              
              beta3 = c(center[2] - sqrt(cfr.fisher * t(C[2, ]) %*% vcov(m0) %*% C[2, ]),
                        center[2],
                        center[2] + sqrt(cfr.fisher * t(C[2, ]) %*% vcov(m0) %*% C[2, ])))
colnames(T2.I) <- c("inf", "center", "sup")
T2.I

# Bonferroni intervals

qT <- qt(1 - alpha/(2*p), df)

BF.I <- cbind(center - sqrt(diag(shape)) * qT,
              center,
              center + sqrt(diag(shape)) * qT)
colnames(BF.I) <- c('inf', 'center', 'sup')
BF.I

# or 

BF.I <- rbind(beta2.4 = c(center[1] - sqrt(shape[1, 1]) * qT,
                          center[1],
                          center[1] + sqrt(shape[1, 1]) * qT),
              
              beta3 = c(center[2] - sqrt(shape[2, 2]) * qT,
                        center[2],
                        center[2] + sqrt(shape[2, 2]) * qT))
colnames(BF.I) <- c("inf", "center", "sup")
BF.I

# or

BF.I <- rbind(beta2.4 = c(center[1] - sqrt(t(C[1, ]) %*% vcov(m0) %*% C[1, ]) * qT,
                          center[1],
                          center[1] + sqrt(t(C[1, ]) %*% vcov(m0) %*% C[1, ]) * qT),
              
              beta3 = c(center[2] - sqrt(t(C[2, ]) %*% vcov(m0) %*% C[2, ]) * qT,
                        center[2],
                        center[2] + sqrt(t(C[2, ]) %*% vcov(m0) %*% C[2, ]) * qT))
colnames(BF.I) <- c("inf", "center", "sup")
BF.I

abline(v = T2.I[1, 1], col = 'orange', lwd = 1, lty = 2)
abline(v = T2.I[1, 3], col = 'orange', lwd = 1, lty = 2)
abline(h = T2.I[2, 1], col = 'orange', lwd = 1, lty = 2)
abline(h = T2.I[2, 3], col = 'orange', lwd = 1, lty = 2)

segments(T2.I[1, 1], 0, T2.I[1, 3], 0, lty = 1, lwd = 2, col = 'orange')
segments(0, T2.I[2, 1], 0, T2.I[2, 3], lty = 1, lwd = 2, col = 'orange')

abline(v = BF.I[1, 1], col = 'purple', lwd = 1, lty = 2)
abline(v = BF.I[1, 3], col = 'purple', lwd = 1, lty = 2)
abline(h = BF.I[2, 1], col = 'purple', lwd = 1, lty = 2)
abline(h = BF.I[2, 3], col = 'purple', lwd = 1, lty = 2)

segments(BF.I[1, 1], 0, BF.I[1, 3], 0, lty = 1, lwd = 2, col = 'purple')
segments(0, BF.I[2, 1], 0, BF.I[2, 3], lty = 1, lwd = 2, col = 'purple')

legend('topright', c('Bonf. CI', 'Sim-T2 CI'), col = c('purple', 'orange'), lty = 1, lwd = 2)

# Confidence Interval for the Variance

S2 <- sum((m0$residuals)^2)/m0$df

k <- 1 # eventual Bonferroni correction

Var.I <- cbind(df * S2 / qchisq(1 - (alpha)/(2*k), df),
               S2,
               df * S2 / qchisq(alpha/(2*k), df))
colnames(Var.I) <- c("inf", "center", "sup")
Var.I


#### Prediction ----

alpha <- 0.05

new.datum <- data.frame(reg1 = 1, reg2 = 2, reg3 = 3, dummy = as.logical("T")) # I think dummy has to be specified as logical when it assumes T/TRUE values

# Conf. int. for the mean
Conf <- predict(m0, new.datum, interval = 'confidence', level = 1 - alpha)  
Conf

# Pred. int. for a new obs
Pred <- predict(m0, new.datum, interval = 'prediction', level = 1 - alpha) 
Pred

Pred[1, "fit"]

range(data[which(data$dummy == "Yes"), ]$reg1)
# Are we far from the data baricenter?


#### Diagnostic ----

par(mfrow=c(2,2))
plot(m0)
par(mfrow=c(1,1))

shapiro.test(m0$residuals)

# Residuals vs. Regressors

par(mfrow=c(2,floor(r/2)+r%%2))

for(i in 1:r) # (correct with (r-1) instead of r if there is a dummy as last regressor, f.e.)
{
  plot(data[, names(m0$coefficients)[i+1]], m0$residuals, xlab = names(m0$coefficients)[i+1], pch = 19)
  abline(h = 0)
}

par(mfrow=c(1,1))
plot(m0$fitted, m0$residuals, pch = 19)

# Collinearity

# Variance Inflation Factor

vif(m0) # Rule of thumb -> problem when VIF exceeds 10 (or 5 sometimes) #!library(car)


##### PCA Regression -----

pc.data <- princomp(cbind(data$reg1, data$reg2, data$reg3), scores = TRUE)
summary(pc.data)
pc.data$load

reg1.pc <- pc.data$scores[, 1]
reg2.pc <- pc.data$scores[, 2]
reg3.pc <- pc.data$scores[, 3]

m1.pc <- lm(target ~ reg1.pc + reg2.pc + reg3.pc + dummy, data = data)

summary(m1.pc)

plot(reg1.pc, data$target, xlab = 'PC1', ylab = 'Target variable', las = 1)
x <- seq(range(reg1.pc)[1], range(reg1.pc)[2], length = 500)
lines(x, m1.pc$coef[1] + m1.pc$coef[2]*x)

# Coefficients of the model which used the original regressors

regressors <- c("reg1_name", "reg2_name", "reg3_name")
means <- colMeans(data[, colnames(data) %in% regressors])
k <- 3 # number of PCs you want to take (<= length(regressors))
# Note: "regressors" are only the ones taking real values (no dummies)

beta0 <- m1.pc$coef[1]
beta <- rep(0, length = length(means))
names(beta) <- names(means)
for(i in 1:length(means))
{
  for(j in 1:k)
  {
    beta0 <- beta0 - m1.pc$coef[j+1] * pc.data$load[i, j] * means[i]
    beta[i] <- beta[i] + m1.pc$coef[j+1] * pc.data$load[i, j]
  }
}
beta0
beta


##### Ridge/Lasso Regression -----

set.seed(20240623)

# Build the matrix of predictors
x <- model.matrix(target ~ reg1 + reg2 + reg3 + dummy, data = data)[, -1]
# Build the vector of response
y <- data$target

# Let's set a grid of candidate lambda's for the estimate
lambda.grid <- 10^seq(1, -2, length = 100)
fit.regularized <- glmnet(x, y, alpha = 1, lambda = lambda.grid) #!library(glmnet)
# Note: "alpha" is the elasticnet mixing parameter, with 0 <= alpha <= 1.
# The penalty is defined as: (1-alpha)/2 * ||beta||2_2 + alpha * ||beta||_1;
# by default, alpha = 1 -> lasso (if alpha = 0 -> ridge regression)

par(mfrow=c(1,1))
plot(fit.regularized, xvar = 'lambda', label = TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col = rainbow(dim(x)[2]), lty = 1, cex = 1)

# Let's set lambda via cross validation
cv.regularized <- cv.glmnet(x, y, alpha = 1, lambda = lambda.grid) # default: 10-fold CV
cv.regularized

bestlam.regularized <- cv.regularized$lambda.min
bestlam.regularized

optlam.regularized <- cv.regularized$lambda.1se # maximum lambda that has its cross-validation mse inside the confidence interval of the mse of lambda.min
optlam.regularized

plot(cv.regularized)
abline(v = log(bestlam.regularized), lty = 1)
abline(v = log(optlam.regularized), lty = 1)

cv.regularized$nzero[which(cv.regularized$lambda == bestlam.regularized)]
cv.regularized$nzero[which(cv.regularized$lambda == optlam.regularized)]
# [What is it best to choose? bestlam or optlam?]
# AFAIK, optlam is usually (always?) bigger than bestlam and therefore performs a more aggressive variable selection
# -> take this one (unless no coefficient is left)

fit.best <- glmnet(x, y, alpha = 1, lambda = optlam.regularized)

coef.best <- predict(fit.regularized, s = bestlam.regularized, type = 'coefficients')[1:(r+1), ]
coef.best[which(coef.best != 0)]

coef.opt <- predict(fit.best, type = 'coefficients')[1:(r+1), ]
coef.opt[which(coef.opt != 0)]

mse.min <- mean((y - predict(fit.regularized, x, s = bestlam.regularized))^2) # Wrong
mse.min <- mean((y - predict(cv.regularized, x, s = bestlam.regularized))^2) # Wrong (same as before)

mse.min <- cv.regularized$cvm[cv.regularized$lambda == bestlam.regularized] # Right
mse.min <- min(cv.regularized$cvm) # Right (same as before)
mse.min

# The discrepancy arises because the mean squared error (MSE) you are 
# calculating using mean((y - fitted_values)^2) is based on the entire dataset, 
# while the MSE reported in the cv.glmnet output is obtained through cross-validation

new.datum <- data.frame(reg1 = 1, reg2 = 2, reg3 = 3, dummy = as.logical("T"))

Pred <- predict(fit.best, as.matrix(new.datum), s = bestlam.regularized, type = "response")
Pred